\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{{Advanced Machine Learning course}\break
{Home assignment 2}}

\author{Yaniv Tal, 031431166}

\begin{document}
\maketitle

\section{Question 1}
    \subsection {part a.}
        Mutual information is defined as the relative entropy between the RVs joint distribution p(x, y) and the product distribution p(x)p(y):
        
        \[ I(X;Y) = \sum_{x\in X,y\in {Y}}p_{(X,Y)}(x,y) \log {\frac {p_{(X,Y)}(x,y)}{p_{X}(x)p_{Y}(y)}} = \]
        \[ \sum_{x\in X, y\in Y} p_{(X,Y)}(x,y) \log {\frac {p_{(X,Y)}(x,y)}{p_{X}(x)}} - \sum _{x\in { {X}},y\in { Y}}p_{(X,Y)}(x,y)\log p_{Y}(y) = \]
        \[ \sum _{x\in { {X}},y\in { {Y}}}p_{X}(x)p_{Y\mid X=x}(y)\log p_{Y\mid X=x}(y) - \sum _{x\in { {X}},y\in { Y}}p_{(X,Y)}(x,y)\log p_{Y}(y) = \]
        \[ \sum _{x\in { {X}}}p_{X}(x)\left(\sum _{y\in { {Y}}}p_{Y\mid X=x}(y)\log p_{Y\mid X=x}(y)\right) - \sum _{y\in {Y}}\left(\sum _{x\in {X}}p_{(X,Y)}(x,y)\right)
        \log p_{Y}(y) = \]
        \[ -\sum _{x\in X}p_{X}(x) H(Y \mid X=x) - \sum _{y\in Y} p_{Y}(y) \log p_{Y}(y) = \]
        \[ -H (Y\mid X) + H(Y) = \] 
        \[ H (Y) - H (Y \mid X) \]
        
        \hfill \\
        Since $I(X;Y)$ is symmetrical for $x, y$, we can do the exact same process with $y$ instead of $x$ and get:
        \[ I(X;Y) = H(X) - H(X \mid Y). \]
        \hfill \\
    
    \break
    \subsection {part b.}
        ** Please note - All Entropy calculations in this paper are using $log_{10}$ base.
        \\
        \hfill \\
        \textbf{i. Required: }
        \[ I(X;Y \mid Z) < I(X;Y) \]

        \hfill \\
        Writing as sums of entropy:
        \[ H(X \mid Z) - H(X \mid Y, Z) < H(X) - H( X \mid Y) \]

        \hfill \\
        An example distribution that fulfills this requirement can be seen in the table below:

        \begin{table}[ht]
        \centering
        \begin{tabular}{a|b|c|d}
        Z & X & Y & Pr(x, y, z) \\
        \hline
        0 & 0 & 0 & 0.1 \\
        0 & 0 & 1 & 0 \\
        0 & 1 & 0 & 0.2 \\
        0 & 1 & 1 & 0 \\
        1 & 0 & 0 & 0 \\
        1 & 0 & 1 & 0.4 \\
        1 & 1 & 0 & 0 \\
        1 & 1 & 1 & 0.3 \\
        \end{tabular}
        \caption{\label{table1} $I(X;Y \mid Z) < I(X;Y)$}
        \end{table}
        
        \hfill \\
        Calculating the above entropy elements one by one:

        \hfill \\
        Entropy of $X$:
        \[ H(X) =  \sum_x p(x) log(p(x)) = \big(-0.5 log(0.5) \big) + \big(-0.5 log(0.5) \big) = 0.301 \]

        \hfill \\
        Entropy of $(X \mid Y)$ - expected to be the same since Y adds no information on X:
        \[ H(X \mid Y) =  \sum_Y p(y) H(X \mid y) = - \sum_y \sum_x p(x, y) log \big(p(x \mid y) \big) =  \]
        \[ \big(-0.1) log(1/3) \big) + \big(-0.2) log(2/3) \big) + \big(-0.4) log(4/7) \big) + \big(-0.3) log(3/7) \big) = 0.29  \]

        \hfill \\
        Entropy of $(X \mid Z)$ - expected to be the same since Z adds no information on X:
        \[ H(X \mid Z) =  \sum_z p(z) H(X \mid z) = - \sum_y \sum_x p(x, z) log \big(p(x \mid z) \big) =  \]
        \[ \big(-0.1) log(1/3) \big) + \big(-0.2) log(2/3) \big) + \big(-0.4) log(4/7) \big) + \big(-0.3) log(3/7) \big) = 0.29  \]
        
        \hfill \\
        Entropy of $(X \mid Y, Z)$ - Expected to be zero since $Y,Z$ define the value of X uniquely:
        \[ H \big (X \mid Y, Z \big) = \sum_y \sum_z p(y, z) H(X \mid y, z) = \]
        \[ -\sum_x \sum_y \sum_z p(x, y, z) log \big(p(x \mid y, z) \big) = \]
        \[ \big(-0.1) log(1/3) \big) + \big(-0.2) log(2/3) \big) + \big(-0.4) log(4/7) \big) + \big(-0.3) log(3/7) \big) = 0.29  \]

        \hfill \\
        \[ I(X;Y \mid Z) = H(X \mid Z) - H(X \mid Y, Z) = 0 \]
        \[ I(X;Y) = H(X) - H( X \mid Y) = 0.217 - 0.217 = 0.011\]
        \textbf{And as required:}
        \[ I(X;Y \mid Z) < I(X;Y) \]
        
    \break
    
        \hfill \\
        \textbf{ii. Now on the opposite direction - Required: }
        \[ I(X;Y \mid Z) > I(X;Y) \]
        Writing as sums of entropy:
        \[ H(X \mid Z) - H(X \mid Y, Z) > H(X) - H( X \mid Y) \]
        \[  \]
        
        \hfill \\
        An example distribution that fulfills this requirement can be seen in the table below:
        \begin{table}[ht]
        \centering
        \begin{tabular}{a|b|c|d}
        Z & X & Y & Pr(x, y, z) \\
        \hline
        0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 0.25 \\
        0 & 1 & 0 & 0.25 \\
        0 & 1 & 1 & 0 \\
        1 & 0 & 0 & 0.25 \\
        1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 0 \\
        1 & 1 & 1 & 0.25 \\
        \end{tabular}
        \caption{\label{table1} $I(X;Y \mid Z) > I(X;Y)$}
        \end{table}
        
        \hfill \\
        Calculating the above entropy elements one by one:

        \hfill \\
        Entropy of $X$:
        \[ H(X) =  \sum_x p(x) log(p(x)) = 2 \cdot \big(-0.5 log(0.5) \big) = 0.301 \]

        \hfill \\
        Entropy of $(X \mid Y)$ - expected to be the same since Y adds no information on X:
        \[ H(X \mid Y) =  \sum_Y p(y) H(X \mid y) = - \sum_y \sum_x p(x, y) log \big(p(x \mid y) \big) =  4 \cdot \big(-0.25 log(0.5) \big) = 0.301 \]

        \hfill \\
        Entropy of $(X \mid Z)$ - expected to be the same since Z adds no information on X:
        \[ H(X \mid Z) =  \sum_z p(z) H(X \mid z) = - \sum_y \sum_x p(x, z) log \big(p(x \mid z) \big) =  4 \cdot \big(-0.25 log(0.5) \big) = 0.301 \]
        
        \hfill \\
        Entropy of $(X \mid Y, Z)$ - Expected to be zero since $Y,Z$ define the value of X uniquely:
        \[ H \big (X \mid Y, Z \big) = \sum_y \sum_z p(y, z) H(X \mid y, z) = \]
        \[ -\sum_x \sum_y \sum_z p(x, y, z) log \big(p(x \mid y, z) \big) = 0 \]

        \hfill \\
        \[ H(X \mid Z) - H(X \mid Y, Z) = 0.301\]
        \[ H(X) - H( X \mid Y) = 0.301 - 0.301 = 0\]
        \textbf{And as required:}
        \[ I(X;Y \mid Z) > I(X;Y) \]



    \break


\section{Question 2}
    Let $X, Y, Z$ three random variables who form a Markov chain $ X \rightarrow Y \rightarrow Z$.
    Based on Bayes:
    \[ p(x, z \mid y) = \dfrac{p(y \mid x,z) p(x, z)}{p(y)} = \dfrac{p(x, y, z)}{p(y)}\]
    
    From the Markovian property of the chain, we get that:
    \[ p(x, y, z) = p(x) p(y \mid x) p(z \mid y)  \]
    
    putting it together and using Bayes to get $p(x \mid y$:
    \[ p(x, z \mid y) = \dfrac{p(x) p(y \mid x) p(z \mid y)}{p(y)} = p(x \mid y) p(z \mid y) \]

    \textbf{As required}

    \break


\section{Question 3}
     \begin{table}[ht]
    \centering
    \begin{tabular}{a|b|c}
    $Pr(x_1, x_2)$ & $x_1$ & $x_2$\\
    \hline
    $y_1$ & 1/4 & 0 \\
    $y_2$ & 1/4 & 1/2 \\
    \end{tabular}
    \caption{\label{table3}}
    \end{table}

    \hfill \\
    \textbf{a. $H(X), H(Y)$}
    \[ H(X) = \sum_x -p(x) log \big(p(x)\big) = -2 \cdot 0.5 log(0.5) = 0.301\]
    \[ H(Y) = \sum_x -p(x) log \big(p(x)\big) = -0.25 log(0.25) - 0.75 log(0.75) = 0.244\]
    
    \hfill \\
    \textbf{b. $H(X \mid Y), H(Y \mid X)$}
    \[ H(X \mid Y) =  \sum_Y p(y) H(X \mid Y) = \sum_Y \sum_X p(x, y) log \big(p(x \mid y) \big) = \]
    \[ -0.25 log(1) - 0 - 0.25 log(0.333) - 0.5 log(0.666) = 0.208  \]
    \\
    \[ H(Y \mid X) =  \sum_X p(x) H(Y \mid X) = \sum_X \sum_Y p(x, y) log \big(p(y \mid x) \big) = \]
    \[ -0.25 log(0.5) - 0.25 log(0.5) - 0 - 0.5 log(1) = 0.151 \]
    
    \hfill \\
    \textbf{c. $H(X, Y)$}
    \[ H(X, Y) = -0.25 log(0.25) -0.25 log(0.25) - 0 - 0.5 log(0.5) = 0.452 \]
    
    \hfill \\
    \textbf{d. $H(X) - H(Y)$} 
    \[ H(X) - H(Y) = 0.301 - 0.244 = 0.057 \]

    \hfill \\
    \textbf{e. $I(X;Y)$} 
    \[ I(X;Y) =  H(X) - H \big( X \mid Y\big) = 0.301 - 0.208 = 0.093 \]
    
    \hfill \\
    \textbf{f. Venn diagram:}     
    TODO Yaniv: insert venn diagram !
    
    \break

\section{Question 4}
    Let 
    \[  p = (p_1, p_2, ..., p_m), \sum p_i = 1 \] \\
    Also, let: 
    \[ q = (q_1, q_2, ..., q_{m-1}), \sum p_i = 1 \] \\
    such that:
    \[ q_1=p_1, q_2 =p_2 \text{        .....        } q_{m-2}=p_{m-2}, q_{m-1} = (p_{m-1}+p_m) \]
    Also let:
    \[ v = \Big( \dfrac{p_{m-1}}{p_{m-1} + p_m}, \dfrac{p_{m}}{p_{m-1} + p_m} \Big) \]\\
    
    \hfill \\
    We need to show that: $H(q) + (p_{m-1} + p_m) H(v) = H(p)$.\\
    \hfill \\
    First, lets look at $H(v)$:
    \[ H(v) = - \Bigg[ \dfrac{p_{m-1}}{p_{m-1} + p_m} log \Big( \dfrac{p_{m-1}}{p_{m-1} + p_m} \Big) + 
    \dfrac{p_{m}}{p_{m-1} + p_m} log \Big( \dfrac{p_{m}}{p_{m-1} + p_m} \Big) \Bigg]= \]
    \[ - \Bigg[ \dfrac{p_{m-1}}{p_{m-1} + p_m} \Big( log(p_{m-1}) - log(p_{m-1 + p_m})  \Big) + 
    \dfrac{p_{m}}{p_{m-1} + p_m} \Big( log(p_{m}) - log(p_{m-1} + p_m)  \Big) \Bigg]= \]
    \[ log(p_{m-1} + p_m) - \dfrac{p_{m-1} log(p_{m-1}) + p_m log(p_m)}{p_{m-1} + p_m}  \]
    \hfill \\
    Now lets look at H(q):
    \[ H(q) = \sum_{i=1}^{m-2} \big(- p_i log(p_i) \big) - (p_{m-1} + p_m) log(p_{m-1} + p_m) \]
    \hfill \\
    Putting it all together:
    \[ H(q) + (p_{m-1} + p_m) H(v) = \]
    \[ \sum_{i=1}^{m-2} \big(- p_i log(p_i) \big) - (p_{m-1} + p_m) log(p_{m-1} + p_m) + 
    (p_{m-1} + p_m) log(p_{m-1} + p_m) - \Big(p_{m-1} log(p_{m-1}) + p_m log(p_m) \Big) =  \]
    \[ \sum_{i=1}^{m-2} \big(- p_i log(p_i) \big) - p_{m-1} log(p_{m-1}) - p_m log(p_m) =  \]
    \[ \sum_{i=1}^{m} \big(- p_i log(p_i) \big)  = H(p) \]
    \textbf{As required}
 
    \break

\section{Question 5}
    Generally, we are looking for distributions $p(x) \neq q(x)$ that will fulfill the following:
    \[ \sum_x -p(x) log \Big( \dfrac{q(x)}{p(x)} \Big) = \sum_x -q(x) log \Big( \dfrac{p(x)}{q(x)} \Big) \]
    I'll choose a Bernully dist. for simplicity, than:
    \[ -p(x) log \Big( \dfrac{q(x)}{p(x)} \Big) - (1-q(x))log \Big( \dfrac{1-q(x)}{1-p(x)} \Big) = -q(x) log \Big( \dfrac{p(x)}{q(x)} \Big) - (1-p(x))log \Big( \dfrac{1-p(x)}{1-q(x)} \Big) \]
    The solutions are either $p(x) = q(x)$, or $p(x) = 1-q(x)$.
    
    \hfill \\
    \textbf{So the example is:  $p \sim Benrully(0.3)$,  $q \sim Benrully(0.7)$}\\
    It is obvious that the following is true:
    
    \[ D(p \mid \mid q) = -0.3 log \Big(\dfrac{0.7}{0.3} \Big) -0.7 log \Big(\dfrac{0.3}{0.7} \Big) = -0.7 log \Big(\dfrac{0.3}{0.7} \Big) -0.3 log \Big(\dfrac{0.7}{0.3} \Big) = D(q \mid \mid p)  \]
    
    
    
    

    \break

\section{Question 6}
    

    \break

\section{Question 7}


    \break

\section{Todo:yaniv:}
1. Done
2. Done
3.f. add venn diagram
4. Done
5. Done
6. Not done
7. Not done

\end{document}